# SFT Configuration for ZIZOGPT (FLATTENED)

# --- Model Arguments ---
# This will be updated by the pipeline script to point to the pre-trained model
model_name_or_path: "./outputs/pretrain/final"
use_lora: true
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target_modules: "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj"
use_flash_attention_2: false

# --- Training Arguments ---
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 8
num_train_epochs: 1
max_steps: 5000  # SFT needs fewer steps than pre-training
learning_rate: 2.0e-5
lr_scheduler_type: "cosine"
warmup_ratio: 0.03
weight_decay: 0.0
max_grad_norm: 1.0
gradient_checkpointing: true
bf16: true
tf32: true
logging_steps: 10
save_steps: 1000
eval_steps: 500
save_total_limit: 3
deepspeed: "configs/deepspeed_zero1.json"
dataloader_num_workers: 8
output_dir: "./outputs/sft"
overwrite_output_dir: true
report_to: "wandb"
run_name: "zizogpt-sft"

# --- Data Arguments ---
# Using standard UltraChat (Open Access)
dataset_subset: "all"
max_seq_length: 2048
streaming: true
num_proc: 16
packing: false
