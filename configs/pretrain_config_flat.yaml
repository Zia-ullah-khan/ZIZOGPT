# Pre-training Configuration for ZIZOGPT (FLATTENED for HfArgumentParser)

# --- Model Arguments ---
from_scratch: true
architecture: "llama"
hidden_size: 2048
intermediate_size: 8192
num_hidden_layers: 16
num_attention_heads: 32
num_key_value_heads: 8
vocab_size: 128256
max_position_embeddings: 131072
rope_theta: 500000.0
model_name_or_path: null
initializer_range: 0.02
use_flash_attention_2: false
tokenizer_path: "meta-llama/Llama-3.1-8B"

# --- Training Arguments ---
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
num_train_epochs: 1
max_steps: 100000 # Increased for full corpus
learning_rate: 3.0e-4
lr_scheduler_type: "cosine"
warmup_steps: 2000
weight_decay: 0.1
max_grad_norm: 1.0
gradient_checkpointing: true
bf16: true
tf32: true
logging_steps: 1
save_steps: 1000
eval_steps: 500
save_total_limit: 5
deepspeed: "configs/deepspeed_zero1.json"
dataloader_num_workers: 16
output_dir: "./outputs/pretrain"
overwrite_output_dir: true
report_to: "wandb"
run_name: "zizogpt-pretrain-streaming"

# --- Data Arguments ---
max_seq_length: 2048
streaming: true
num_proc: 16
use_sample_dataset: false
dataset_weights: "0.5,0.2,0.15,0.1,0.05"
