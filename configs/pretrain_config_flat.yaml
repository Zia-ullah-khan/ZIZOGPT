# Pre-training Configuration for ZIZOGPT (FLATTENED for HfArgumentParser)

# --- Model Arguments ---
architecture: "llama"
hidden_size: 2048
intermediate_size: 5504
num_hidden_layers: 24
num_attention_heads: 16
num_key_value_heads: 16
vocab_size: 32768
max_position_embeddings: 262144
rope_theta: 10000.0
pretrained_model_name_or_path: null
initializer_range: 0.02
use_flash_attention_2: false
tokenizer_path: "./tokenizer"

# --- Training Arguments ---
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
num_train_epochs: 1
max_steps: -1
learning_rate: 3.0e-4
lr_scheduler_type: "cosine"
warmup_steps: 1000
weight_decay: 0.1
max_grad_norm: 1.0
gradient_checkpointing: true
bf16: true
tf32: true
logging_steps: 10
save_steps: 1000
eval_steps: 500
save_total_limit: 3
deepspeed: "configs/deepspeed_zero1.json"
dataloader_num_workers: 16
output_dir: "./outputs/pretrain"
overwrite_output_dir: true
report_to: "wandb"
run_name: "zizogpt-pretrain"

# --- Data Arguments ---
max_seq_length: 2048
streaming: true
num_proc: 16
# Dataset weights passed as string since argparse expects it
dataset_weights: "0.5,0.2,0.15,0.1,0.05"
