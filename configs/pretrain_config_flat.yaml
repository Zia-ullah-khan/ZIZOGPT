# Optimized for 4x B200 Scratch Build - VERIFIED FOR STREAMING
# Using FineWeb-Edu (10 Billion Tokens)

# --- Model Arguments ---
from_scratch: true
architecture: "llama"
hidden_size: 2048
intermediate_size: 8192
num_hidden_layers: 16
num_attention_heads: 32
num_key_value_heads: 8
vocab_size: 128256
max_position_embeddings: 8192
rope_theta: 500000.0
model_name_or_path: null
initializer_range: 0.02
use_flash_attention_2: false
tokenizer_path: "Xenova/llama-3-tokenizer"

# --- Training Arguments ---
output_dir: "./outputs/pretrain"
per_device_train_batch_size: 64
per_device_eval_batch_size: 64
gradient_accumulation_steps: 1
num_train_epochs: 1
max_steps: 100000 
learning_rate: 3.0e-4
lr_scheduler_type: "cosine"
warmup_steps: 2000
weight_decay: 0.1
max_grad_norm: 1.0
gradient_checkpointing: true
bf16: true
tf32: true
logging_steps: 5
save_steps: 500
eval_steps: 500
save_total_limit: 5
deepspeed: "configs/deepspeed_zero1.json"
dataloader_num_workers: 16
overwrite_output_dir: true
report_to: "wandb"
run_name: "zizogpt-pretrain-fineweb"

# --- Data Arguments ---
dataset_name: "HuggingFaceFW/fineweb-edu"
dataset_config_name: "sample-10BT"
max_seq_length: 2048
streaming: true
num_proc: 16
use_sample_dataset: false