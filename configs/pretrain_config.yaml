# Pre-training Configuration for ZIZOGPT
# Using NVIDIA Nemotron Pre-Training Datasets

model:
  # Base model architecture - you can start from scratch or from a pretrained checkpoint
  architecture: "llama"  # Options: llama, mistral, gpt2
  hidden_size: 2048
  intermediate_size: 5504
  num_hidden_layers: 24
  num_attention_heads: 16
  num_key_value_heads: 16
  vocab_size: 128000
  max_position_embeddings: 262144
  rope_theta: 10000.0
  
  # If you want to start from an existing model checkpoint
  pretrained_model_name_or_path: null  # e.g., "meta-llama/Llama-2-7b-hf"
  
  # Model initialization
  initializer_range: 0.02
  use_flash_attention_2: true
  tokenizer_path: "./tokenizer" # Path to the trained tokenizer

# Nemotron Pre-Training Datasets
datasets:
  # Main pre-training datasets (choose based on your compute budget)
  pretraining:
    - name: "nvidia/Nemotron-CC-v2.1"
      weight: 0.5
      streaming: true
      split: "train"
      
    - name: "nvidia/Nemotron-Pretraining-Code-v2"
      weight: 0.2
      streaming: true
      split: "train"
      
    - name: "nvidia/Nemotron-CC-Math-v1"
      weight: 0.15
      streaming: true
      split: "train"
      
    - name: "nvidia/Nemotron-Pretraining-Specialized-v1"
      weight: 0.1
      streaming: true
      split: "train"
      
    - name: "nvidia/Nemotron-CC-Code-v1"
      weight: 0.05
      streaming: true
      split: "train"
  
  # Smaller sample for testing
  sample:
    - name: "nvidia/Nemotron-Pretraining-Dataset-sample"
      weight: 1.0
      streaming: false
      split: "train"

training:
  # Training hyperparameters
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  max_steps: -1  # -1 means use epochs
  
  # Learning rate schedule
  learning_rate: 3.0e-4
  lr_scheduler_type: "cosine"
  warmup_steps: 1000
  weight_decay: 0.1
  
  # Gradient settings
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  # Precision
  bf16: true
  tf32: true
  
  # Sequence length
  max_seq_length: 2048
  
  # Logging
  logging_steps: 10
  save_steps: 1000
  eval_steps: 500
  save_total_limit: 3
  
  # Distributed training
  deepspeed: "configs/deepspeed_zero3.json"
  
  # Output
  output_dir: "./outputs/pretrain"
  overwrite_output_dir: true
  
  # Wandb
  report_to: "wandb"
  run_name: "zizogpt-pretrain"

tokenizer:
  # Tokenizer settings
  model_max_length: 4096
  padding_side: "right"
  use_fast: true
  
  # If training a new tokenizer
  train_new_tokenizer: false
  vocab_size: 32000
