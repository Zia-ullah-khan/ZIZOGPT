# Reinforcement Learning Configuration for ZIZOGPT
# Using NVIDIA Nemotron RL Training Blend

model:
  # Path to SFT model
  model_name_or_path: "./outputs/sft/final"
  
  # Reference model for KL divergence
  ref_model_name_or_path: "./outputs/sft/final"
  
  # Reward model
  reward_model_name_or_path: null  # Will use a verifier-based reward
  
  # LoRA for efficient RL
  use_lora: true
  lora_config:
    r: 32
    lora_alpha: 64
    lora_dropout: 0.05
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"

# RL Datasets
datasets:
  rl:
    - name: "nvidia/Nemotron-3-Nano-RL-Training-Blend"
      split: "train"
      streaming: false

# GRPO/DPO Configuration
rl_algorithm: "dpo"  # Options: dpo, ppo, grpo

dpo:
  beta: 0.1  # KL penalty coefficient
  loss_type: "sigmoid"  # Options: sigmoid, hinge, ipo
  label_smoothing: 0.0
  
ppo:
  # PPO-specific settings
  ppo_epochs: 4
  mini_batch_size: 4
  init_kl_coef: 0.2
  target_kl: 6.0
  reward_baseline: 0.0
  
grpo:
  # GRPO-specific settings (Group Relative Policy Optimization)
  num_generations: 4
  temperature: 0.7
  kl_coef: 0.05

training:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  num_train_epochs: 1
  max_steps: -1
  
  learning_rate: 5.0e-7
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 0.0
  
  max_grad_norm: 1.0
  gradient_checkpointing: true
  
  bf16: true
  tf32: true
  
  max_seq_length: 2048
  max_prompt_length: 1024
  max_response_length: 1024
  
  logging_steps: 10
  save_steps: 200
  eval_steps: 100
  save_total_limit: 3
  
  output_dir: "./outputs/rl"
  overwrite_output_dir: true
  
  report_to: "wandb"
  run_name: "zizogpt-rl"
