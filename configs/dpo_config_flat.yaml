# DPO Configuration for ZIZOGPT (FLATTENED)

# --- Model Arguments ---
# This will be updated by the pipeline script to point to the merged SFT model
model_name_or_path: "./outputs/sft/merged"
ref_model_name_or_path: "./outputs/sft/merged"
use_lora: true
lora_r: 32
lora_alpha: 64
lora_dropout: 0.05
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"
use_flash_attention_2: false

# --- RL Arguments ---
rl_algorithm: "dpo"
beta: 0.1
loss_type: "sigmoid"
label_smoothing: 0.0

# --- Training Arguments ---
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 16
num_train_epochs: 1
max_steps: 1000
learning_rate: 5.0e-7
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
weight_decay: 0.0
max_grad_norm: 1.0
gradient_checkpointing: true
bf16: true
tf32: true
logging_steps: 10
save_steps: 200
eval_steps: 100
save_total_limit: 3
output_dir: "./outputs/rl"
overwrite_output_dir: true
report_to: "wandb"
run_name: "zizogpt-dpo"

# --- Data Arguments ---
# Using UltraFeedback (Open Access)
dataset_name: "HuggingFaceH4/ultrafeedback_binarized"
max_seq_length: 2048
max_prompt_length: 1024
max_response_length: 1024
num_proc: 16
